# Code 401 - Read 42 - Ethics

## Amazon Workers Demand Jeff Bezos Cancel Face Recognition Contracts With Law Enforcement -> for Ethics in the workplace

In 2018, workers at Amazon stood up against selling the company’s Rekognition facial recognition software to law enforcement and to boot the data-mining firm Palantir from its cloud services.

An investigation by the American Civil Liberties Union revealed that Amazon had heavily marketed its Rekognition software to police departments and government agencies. The technology can recognize and track faces in real time, and the ACLU noted that such a powerful surveillance tool could easily be misused by law enforcement and it will be a unique threat to civil rights and especially to the immigrants and people of color under attack by the government.

The employees sent a letter to their CEO Jeff Bezos, stating that their company should not be in the surveillance business; it should not be in the policing business; it should not be in the business of supporting those who monitor and oppress marginalized populations.

Earlier that month, Google employees saw the success of internal activism against their company’s artificial intelligence contract with the Pentagon—at a staff meeting in June, Google Cloud CEO Diane Greene announced that the company would not renew its contract for Project Maven, a Defense Department pilot program that uses artificial intelligence to analyze drone footage. Google also announced new ethics principles to govern its artificial intelligence work.

It’s not clear yet whether the employee campaigns at Amazon will have the same impact as the employee activism at Google, but the pushback at shows that tech employees are increasingly willing to speak up against work that they believe to be unethical.

-> reflections: it is important for employees to ensure that services provided by their companies are used in an ethical manner that does not harm people, and they should speak up and refuse anything that is unethical.

## BIG DATA IS OUR GENERATION’S CIVIL RIGHTS ISSUE, AND WE DON’T KNOW IT -> for Ethics in Technology

"Data doesn’t invade people’s lives. Lack of control over how it’s used does."

In the old, data-is-scarce model, companies had to decide what to collect first, and then collect it. With the new, data-is-abundant model,  data is collected first and questions are asked later. This means that information is being collected long before deciding what it’s for, which can turn out to be dangerous.

There are brilliant examples of how this approach can improve the way we live. For example, big data can be used to detect disease outbreaks, improve how students learn, reveal political partisanship. But it can turn to a civil rights issue. For instance, if information on the music you listen to was collected, you might assume it will be used to suggest new songs for ypu, or to share it with your friends. But instead, it can be used to guess racial background, or to deny you a loan!

Having an enormous amount of data about anyone, and being able to analyze it without censorship makes private information like your gender, race, religion, and sexual orientation exposed and unprotected, and it might be used against you.

The only way to deal with this properly is to somehow link what the data is with how it can be used. I might, for example, say that my musical tastes should be used for song recommendation, but not for banking decisions.

Tying data to permissions can be done through encryption, which is slow, riddled with DRM, burdensome, hard to implement, and bad for innovation. Or it can be done through legislation, which has about as much chance of success as regulating spam: it feels great, but it’s damned hard to enforce.

-> reflections: there should be control over technology to avoid any unethical violations and harm caused to people.
